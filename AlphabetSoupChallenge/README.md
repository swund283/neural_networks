# neural_networks
This was a fun one.  In the end, i was able to able to use 2 hidden layers of 80 and 30 to get the NN modle to yield results over .75 accuracy.  This took a number of efforts.  I left some 'residual' data/syntax for refrence.  But, basically i had a number of verions with 1-3 hidden layers where i 1) changed the # of neurons for each layer 2) attempted to use different activations.  In addition to adjusting the # of hidden layers and the makeup of each layer.  i also supressed some dup data upstream that was redundant.  Lastly, i also built a variable for the optimization layer instead of just hardcoding in as "adam", in the end i did use default params; but, was able to play with things like the learning rate using this method.

In addition, i was able to use random forest to get a accuracy value of over .75.   Looking back at this effort - i feel random forest was a better tact to achieving this goal.  it was less work, which is alaway nice when you get similar results.  But, using random forest you can still see the 'original' inputs for further analysis where as in a NN these are lost in the weights of the layers.
